{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Style_Transfer_Modularizada.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AtVn_nSImk5",
        "colab_type": "code",
        "outputId": "556d6d22-0f71-4e8a-b183-2e7a52cd7db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Aug 23 18:05:15 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.40       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   73C    P0    70W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwedatj5WJq5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "96e6df80-e5b6-4a6d-ed15-6343138d06dd"
      },
      "source": [
        "from keras.preprocessing.image import load_img, save_img, img_to_array\n",
        "from scipy.optimize import fmin_l_bfgs_b\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from keras.applications import vgg19\n",
        "from keras import backend as K"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTW1_iKCWcwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_image_path = '/content/puc.jpg'\n",
        "style_reference_image_path = '/content/van_gogh.jpg'\n",
        "result_prefix = 'b'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OLPozUaWkKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iterations = 10\n",
        "# these are the weights of the different loss components\n",
        "total_variation_weight = 1.0 \n",
        "style_weight = 1.0 \n",
        "content_weight = 0.025"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onXCeXe3WkeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dimensions of the generated picture - redimension to fixed height (400) and \n",
        "# proportional width\n",
        "width, height = load_img(base_image_path).size\n",
        "img_nrows = 400\n",
        "img_ncols = int(width * img_nrows / height)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtpqnNAzWkro",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util function to open, resize and format pictures into appropriate tensors\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = load_img(image_path, target_size=(img_nrows, img_ncols))\n",
        "    img = img_to_array(img)\n",
        "    # add +1 dimension on array for working with batch of images\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    # preprocesses an numpy or tensor array encoding a batch of images.\n",
        "    # will convert the images from RGB to BGR,\n",
        "    # then will zero-center each color channel with\n",
        "    # respect to the ImageNet dataset,\n",
        "    # without scaling\n",
        "    img = vgg19.preprocess_input(img)\n",
        "    return img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFGzxVgOWpN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# util function to convert a tensor into a valid image\n",
        "\n",
        "def deprocess_image(x):\n",
        "    if K.image_data_format() == 'channels_first':                               \n",
        "        x = x.reshape((3, img_nrows, img_ncols))\n",
        "        x = x.transpose((1, 2, 0))\n",
        "    else:\n",
        "        x = x.reshape((img_nrows, img_ncols, 3))\n",
        "    # Remove zero-center by mean pixel\n",
        "    x[:, :, 0] += 103.939\n",
        "    x[:, :, 1] += 116.779\n",
        "    x[:, :, 2] += 123.68\n",
        "    # 'BGR'->'RGB'\n",
        "    x = x[:, :, ::-1]\n",
        "    x = np.clip(x, 0, 255).astype('uint8')\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPs9wGHjWsVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get tensor representations of our images\n",
        "base_image = K.variable(preprocess_image(base_image_path))\n",
        "style_reference_image = K.variable(preprocess_image(style_reference_image_path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjNgPOaye8-5",
        "colab_type": "code",
        "outputId": "a4357c6c-66ed-43fa-f284-d8ba20491252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# this will contain our generated image\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    combination_image = K.placeholder((1, 3, img_nrows, img_ncols))\n",
        "else:\n",
        "    combination_image = K.placeholder((1, img_nrows, img_ncols, 3))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0823 18:05:18.021585 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJOFFeK5bEQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# combine the 3 images into a single Keras tensor\n",
        "input_tensor = K.concatenate([base_image,\n",
        "                              style_reference_image,\n",
        "                              combination_image], axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKit16NAbEef",
        "colab_type": "code",
        "outputId": "5a0d5a7d-9dc7-4a5d-9cb6-6de2c5ef89f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        }
      },
      "source": [
        "# build the VGG19 network with our 3 images as input\n",
        "# the model will be loaded with pre-trained ImageNet weights.\n",
        "# we also scrap the top layers. \n",
        "# the top layers are the layers that involve flattening the network so that \n",
        "# we can create a couple dense layers that output classification.\n",
        "# we don't need these here, we only need layers involving convolutions, \n",
        "# so we toss those top layers away\n",
        "model = vgg19.VGG19(input_tensor=input_tensor,\n",
        "                    weights='imagenet', include_top=False)\n",
        "\n",
        "print('Model loaded.')\n",
        "\n",
        "model.layers"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 18:05:18.055385 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0823 18:05:18.058183 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0823 18:05:18.100676 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0823 18:05:18.730839 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0823 18:05:18.732833 140117651953536 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model loaded.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<keras.engine.input_layer.InputLayer at 0x7f6f6029c710>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f6028ec88>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f6028ea90>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f6f8b437320>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f602a5ef0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f9fd6d8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f6f5fa19a90>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5fa192e8>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f9c3550>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f9c3f28>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f975e48>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f6f5f9a7048>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f988978>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f9401d0>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f952a20>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f8eaef0>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f6f5f91be10>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f91b400>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f8ce320>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f8e4c88>,\n",
              " <keras.layers.convolutional.Conv2D at 0x7f6f5f8789e8>,\n",
              " <keras.layers.pooling.MaxPooling2D at 0x7f6f5f892e48>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLwMVBqFdnPE",
        "colab_type": "text"
      },
      "source": [
        "# VGG19 Model\n",
        "\n",
        "#![Image of VGG19](https://www.researchgate.net/profile/Clifford_Yang/publication/325137356/figure/fig2/AS:670371271413777@1536840374533/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffbH9uECZxAw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "3b7cfef8-3366-4f2d-d5c5-46e7c566192d"
      },
      "source": [
        "# get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
        "outputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\n",
        "outputs_dict"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'block1_conv1': <tf.Tensor 'block1_conv1/Relu:0' shape=(3, 400, 615, 64) dtype=float32>,\n",
              " 'block1_conv2': <tf.Tensor 'block1_conv2/Relu:0' shape=(3, 400, 615, 64) dtype=float32>,\n",
              " 'block1_pool': <tf.Tensor 'block1_pool/MaxPool:0' shape=(3, 200, 307, 64) dtype=float32>,\n",
              " 'block2_conv1': <tf.Tensor 'block2_conv1/Relu:0' shape=(3, 200, 307, 128) dtype=float32>,\n",
              " 'block2_conv2': <tf.Tensor 'block2_conv2/Relu:0' shape=(3, 200, 307, 128) dtype=float32>,\n",
              " 'block2_pool': <tf.Tensor 'block2_pool/MaxPool:0' shape=(3, 100, 153, 128) dtype=float32>,\n",
              " 'block3_conv1': <tf.Tensor 'block3_conv1/Relu:0' shape=(3, 100, 153, 256) dtype=float32>,\n",
              " 'block3_conv2': <tf.Tensor 'block3_conv2/Relu:0' shape=(3, 100, 153, 256) dtype=float32>,\n",
              " 'block3_conv3': <tf.Tensor 'block3_conv3/Relu:0' shape=(3, 100, 153, 256) dtype=float32>,\n",
              " 'block3_conv4': <tf.Tensor 'block3_conv4/Relu:0' shape=(3, 100, 153, 256) dtype=float32>,\n",
              " 'block3_pool': <tf.Tensor 'block3_pool/MaxPool:0' shape=(3, 50, 76, 256) dtype=float32>,\n",
              " 'block4_conv1': <tf.Tensor 'block4_conv1/Relu:0' shape=(3, 50, 76, 512) dtype=float32>,\n",
              " 'block4_conv2': <tf.Tensor 'block4_conv2/Relu:0' shape=(3, 50, 76, 512) dtype=float32>,\n",
              " 'block4_conv3': <tf.Tensor 'block4_conv3/Relu:0' shape=(3, 50, 76, 512) dtype=float32>,\n",
              " 'block4_conv4': <tf.Tensor 'block4_conv4/Relu:0' shape=(3, 50, 76, 512) dtype=float32>,\n",
              " 'block4_pool': <tf.Tensor 'block4_pool/MaxPool:0' shape=(3, 25, 38, 512) dtype=float32>,\n",
              " 'block5_conv1': <tf.Tensor 'block5_conv1/Relu:0' shape=(3, 25, 38, 512) dtype=float32>,\n",
              " 'block5_conv2': <tf.Tensor 'block5_conv2/Relu:0' shape=(3, 25, 38, 512) dtype=float32>,\n",
              " 'block5_conv3': <tf.Tensor 'block5_conv3/Relu:0' shape=(3, 25, 38, 512) dtype=float32>,\n",
              " 'block5_conv4': <tf.Tensor 'block5_conv4/Relu:0' shape=(3, 25, 38, 512) dtype=float32>,\n",
              " 'block5_pool': <tf.Tensor 'block5_pool/MaxPool:0' shape=(3, 12, 19, 512) dtype=float32>,\n",
              " 'input_1': <tf.Tensor 'concat:0' shape=(3, 400, 615, 3) dtype=float32>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3qnVo3xiVVl",
        "colab_type": "text"
      },
      "source": [
        "# Compute the neural style loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Jpcy4nlqHXI",
        "colab_type": "text"
      },
      "source": [
        "## Gram Matrix\n",
        "is the dot product (aka \"produto escalar\") between the feature matrix and its transpose.\n",
        "\n",
        "![Gram matrix calculation](http://ankitmathur.me/classes/final_files/image04.jpg)\n",
        "\n",
        "The terms of this matrix are proportional to the covariances of corresponding sets of features, and thus captures information about which features tend to activate together. By only capturing these aggregate statistics across the image, they are blind to the specific arrangement of objects inside the image. **This is what allows them to capture information about style independent of content.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx9W8Lp0qGiB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the gram matrix of an image tensor (feature-wise outer product)\n",
        "\n",
        "def gram_matrix(x):\n",
        "    assert K.ndim(x) == 3\n",
        "    if K.image_data_format() == 'channels_first':                                \n",
        "        features = K.batch_flatten(x)\n",
        "    else:\n",
        "        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))    \n",
        "    gram = K.dot(features, K.transpose(features)) \n",
        "    return gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTz7Htu4q14D",
        "colab_type": "text"
      },
      "source": [
        "## Style Loss\n",
        "\n",
        "is the (scaled, squared) Frobenius norm of the difference between the Gram matrices of the style and combination images.\n",
        "\n",
        "![Style loss](http://ankitmathur.me/classes/final_files/image05.jpg)\n",
        "\n",
        "\n",
        "This function is designed to **maintain the style of the reference image in the generated image**.\n",
        "It's based on the gram matrices (which capture style) of feature maps from the style reference image and from the generated image.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MZ4N8DWuJCG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def style_loss(style, combination):\n",
        "    assert K.ndim(style) == 3\n",
        "    assert K.ndim(combination) == 3\n",
        "    S = gram_matrix(style)\n",
        "    C = gram_matrix(combination)\n",
        "    channels = 3\n",
        "    size = img_nrows * img_ncols\n",
        "    return K.sum(K.square(S - C)) / (4.0 * (channels ** 2) * (size ** 2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADyxVnlsuJ6n",
        "colab_type": "text"
      },
      "source": [
        "## Content Loss\n",
        "\n",
        "is the (scaled, squared) Euclidean distance between feature representations of the content and combination images.\n",
        "\n",
        "![Content loss equation](http://ankitmathur.me/classes/final_files/image02.jpg)\n",
        "\n",
        "This function is designed to **maintain the \"content\" of the base image in the generated image**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vLGB0g4LWsYA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def content_loss(base, combination):\n",
        "    return K.sum(K.square(combination - base))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPjMzFQzdaR-",
        "colab_type": "text"
      },
      "source": [
        "## Total Variation Loss\n",
        "\n",
        "was designed to keep the generated image locally coherent, encouraging spatial smoothness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgpbtEKN4ABQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def total_variation_loss(x):\n",
        "    assert K.ndim(x) == 4\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        a = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, 1:, :img_ncols - 1])\n",
        "        b = K.square(\n",
        "            x[:, :, :img_nrows - 1, :img_ncols - 1] - x[:, :, :img_nrows - 1, 1:])\n",
        "    else:\n",
        "        a = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, 1:, :img_ncols - 1, :])\n",
        "        b = K.square(\n",
        "            x[:, :img_nrows - 1, :img_ncols - 1, :] - x[:, :img_nrows - 1, 1:, :])\n",
        "    return K.sum(K.pow(a + b, 1.25))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWoZLbEKjmuu",
        "colab_type": "text"
      },
      "source": [
        "## Combining Loss Functions\n",
        "\n",
        "So now it's time to merge all the loss functions together. **We'll assign each of them a weight that determines how much each will influence the overall cost**. Assigning heavier loss to the style will result in more style, whereas heavier content loss will result in the output being more true to the original content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3jT_sMRWsai",
        "colab_type": "code",
        "outputId": "ec91e802-8cfb-47ab-8863-9c8f53c3d80d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "loss = K.variable(0.0)\n",
        "layer_features = outputs_dict['block5_conv2']\n",
        "\n",
        "# content loss just gets the content for the two features for calculating\n",
        "base_image_features = layer_features[0, :, :, :]\n",
        "combination_features = layer_features[2, :, :, :]\n",
        "loss += content_weight * content_loss(base_image_features,\n",
        "                                      combination_features)\n",
        "\n",
        "# style loss actually iterates through the layers, and multiplies by the \n",
        "# weighting per layer here\n",
        "feature_layers = ['block1_conv1', 'block2_conv1',\n",
        "                  'block3_conv1', 'block4_conv1',\n",
        "                  'block5_conv1']\n",
        "for layer_name in feature_layers:\n",
        "    layer_features = outputs_dict[layer_name]\n",
        "    style_reference_features = layer_features[1, :, :, :]\n",
        "    combination_features = layer_features[2, :, :, :]\n",
        "    sl = style_loss(style_reference_features, combination_features)\n",
        "    loss += (style_weight / len(feature_layers)) * sl\n",
        "    \n",
        "# total loss\n",
        "loss += total_variation_weight * total_variation_loss(combination_image) "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 18:05:19.747708 140117651953536 variables.py:2429] Variable += will be deprecated. Use variable.assign_add if you want assignment to the variable value or 'x = x + y' if you want a new python Tensor object.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-sfk8Ddj1ON",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "35c12f23-218a-4f0c-8af9-1f0545cb19d9"
      },
      "source": [
        "# define gradients of the total loss relative to the combination image, \n",
        "# and use these gradients to iteratively improve upon our combination image \n",
        "# to minimise the loss\n",
        "\n",
        "grads = K.gradients(loss, combination_image)\n",
        "\n",
        "outputs = [loss]\n",
        "if isinstance(grads, (list, tuple)):\n",
        "    outputs += grads\n",
        "else:\n",
        "    outputs.append(grads)\n",
        "\n",
        "f_outputs = K.function([combination_image], outputs)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0823 18:05:20.029025 140117651953536 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1205: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0zjF_jBWsdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_loss_and_grads(x):\n",
        "    if K.image_data_format() == 'channels_first':\n",
        "        x = x.reshape((1, 3, img_nrows, img_ncols))\n",
        "    else:\n",
        "        x = x.reshape((1, img_nrows, img_ncols, 3))\n",
        "    outs = f_outputs([x])\n",
        "    loss_value = outs[0]\n",
        "    if len(outs[1:]) == 1:\n",
        "        grad_values = outs[1].flatten().astype('float64')\n",
        "    else:\n",
        "        grad_values = np.array(outs[1:]).flatten().astype('float64')\n",
        "    return loss_value, grad_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzD_twnXFJk6",
        "colab_type": "text"
      },
      "source": [
        "## ``Evaluator`` class\n",
        "\n",
        "makes it possible to compute loss and gradients in one pass while retrieving them via two separate functions, \"loss\" and \"grads\". This is done because ``scipy.optimize`` requires separate functions for loss and gradients, but computing them separately would be inefficient."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qFE3OhCWsfv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Evaluator(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.loss_value = None\n",
        "        self.grads_values = None\n",
        "\n",
        "    def loss(self, x):\n",
        "        assert self.loss_value is None\n",
        "        loss_value, grad_values = eval_loss_and_grads(x)\n",
        "        self.loss_value = loss_value\n",
        "        self.grad_values = grad_values\n",
        "        return self.loss_value\n",
        "\n",
        "    def grads(self, x):\n",
        "        assert self.loss_value is not None\n",
        "        grad_values = np.copy(self.grad_values)\n",
        "        self.loss_value = None\n",
        "        self.grad_values = None\n",
        "        return grad_values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiFhVVfiW7Wh",
        "colab_type": "code",
        "outputId": "09e1e6b0-3e28-4847-9bc9-2647012e69e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "evaluator = Evaluator()\n",
        "\n",
        "# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n",
        "# so as to minimize the neural style loss\n",
        "x = preprocess_image(base_image_path)\n",
        "\n",
        "for i in range(iterations):\n",
        "    print('Start of iteration', i)\n",
        "    start_time = time.time()\n",
        "    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n",
        "                                     fprime=evaluator.grads, maxfun=20)\n",
        "    print('Current loss value:', min_val)\n",
        "    # save current generated image\n",
        "    img = deprocess_image(x.copy())\n",
        "    fname = result_prefix + '_at_iteration_%d.png' % i\n",
        "    save_img(fname, img)\n",
        "    end_time = time.time()\n",
        "    print('Image saved as', fname)\n",
        "    print('Iteration %d completed in %ds' % (i, end_time - start_time))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start of iteration 0\n",
            "Current loss value: 4132342300.0\n",
            "Image saved as b_at_iteration_0.png\n",
            "Iteration 0 completed in 22s\n",
            "Start of iteration 1\n",
            "Current loss value: 2777454000.0\n",
            "Image saved as b_at_iteration_1.png\n",
            "Iteration 1 completed in 17s\n",
            "Start of iteration 2\n",
            "Current loss value: 2350677500.0\n",
            "Image saved as b_at_iteration_2.png\n",
            "Iteration 2 completed in 17s\n",
            "Start of iteration 3\n",
            "Current loss value: 2169016000.0\n",
            "Image saved as b_at_iteration_3.png\n",
            "Iteration 3 completed in 17s\n",
            "Start of iteration 4\n",
            "Current loss value: 2068068200.0\n",
            "Image saved as b_at_iteration_4.png\n",
            "Iteration 4 completed in 17s\n",
            "Start of iteration 5\n",
            "Current loss value: 1995752000.0\n",
            "Image saved as b_at_iteration_5.png\n",
            "Iteration 5 completed in 17s\n",
            "Start of iteration 6\n",
            "Current loss value: 1951018100.0\n",
            "Image saved as b_at_iteration_6.png\n",
            "Iteration 6 completed in 17s\n",
            "Start of iteration 7\n",
            "Current loss value: 1914645100.0\n",
            "Image saved as b_at_iteration_7.png\n",
            "Iteration 7 completed in 17s\n",
            "Start of iteration 8\n",
            "Current loss value: 1884814300.0\n",
            "Image saved as b_at_iteration_8.png\n",
            "Iteration 8 completed in 17s\n",
            "Start of iteration 9\n",
            "Current loss value: 1859737600.0\n",
            "Image saved as b_at_iteration_9.png\n",
            "Iteration 9 completed in 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_LtJYEg4mjy",
        "colab_type": "text"
      },
      "source": [
        "## Reference \n",
        "\n",
        "https://www.researchgate.net/figure/llustration-of-the-network-architecture-of-VGG-19-model-conv-means-convolution-FC-means_fig2_325137356\n",
        "\n",
        "https://machinelearningmastery.com/a-gentle-introduction-to-channels-first-and-channels-last-image-formats-for-deep-learning/\n",
        "\n",
        "https://towardsdatascience.com/neural-networks-intuitions-2-dot-product-gram-matrix-and-neural-style-transfer-5d39653e7916\n",
        "\n",
        "https://machinelearningmastery.com/how-to-load-convert-and-save-images-with-the-keras-api/\n",
        "\n",
        "https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg19.py\n",
        "\n",
        "https://github.com/hunter-heidenreich/ML-Open-Source-Implementations\n",
        "\n",
        "https://www.quora.com/What-is-the-Gram-Matrix-of-a-set-of-images\n",
        "\n",
        "https://harishnarayanan.org/writing/artistic-style-transfer/\n",
        "\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras\n",
        "\n",
        "https://keras.io/examples/neural_style_transfer/"
      ]
    }
  ]
}